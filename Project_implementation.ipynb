{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of ANN:\n",
    "\n",
    "## Initializing the ANN for a given architecture:\n",
    "\n",
    "- $W$ and $b$ are the wights and biases for each layer respectively.\n",
    "- $Z$ are the input into each neuron before activation.\n",
    "- $A$ are the output of each neuron after activation \n",
    "\n",
    "Initialization:\n",
    "\n",
    "- ANN_architecture is a dictionary containing the number of neurons in each layer and their activation functions.\n",
    "- $W_l$ is initialized randomly to a size of $n_{l+1} \\times n_l$.\n",
    "- $b_l$ is initialized as zero of size $n_{l+1} \\times 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries.  \n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization.\n",
    "\n",
    "def init_ANN(ANN_architecture, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Initialize parameters.\n",
    "    parameters = {}\n",
    "\n",
    "    for idx, layer in enumerate(ANN_architecture):\n",
    "        layer_idx = idx+1\n",
    "        \n",
    "        input_layer_size = layer['inputs']\n",
    "        output_layer_size = layer['outputs']\n",
    "\n",
    "        parameters['W' + str(layer_idx)] = random.randn(output_layer_size, input_layer_size)\n",
    "        parameters['b' + str(layer_idx)] = np.zeros((output_layer_size, 1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Error function. (Mean Squared Error)\n",
    "\n",
    "def error_fn(Y_hat, Y):\n",
    "    N = Y_hat.shape[1]\n",
    "    return np.squeeze((1/(2*N))*np.dot((Y_hat - Y), (Y_hat - Y).T))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation:\n",
    "\n",
    "- $Z_l = W_{(l,l-1)}A_{l-1} + b_{l-1}$ \n",
    "- $A_l = sigmoid(Z_l)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions.\n",
    "\n",
    "def linear(Z):\n",
    "    A = Z\n",
    "    return A\n",
    "\n",
    "def RelU(Z):\n",
    "    A = Z\n",
    "    A[A < 0] = 0\n",
    "    return A\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def activation_fn(Z, activation):\n",
    "    if activation == 'linear':\n",
    "        return linear(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        return sigmoid(Z)\n",
    "    elif activation == 'ReLU':\n",
    "        return RelU(Z)\n",
    "    else:\n",
    "        print('Activation function not recognized.')\n",
    "\n",
    "\n",
    "# Derivatives of activation functions.\n",
    "\n",
    "def lin_der(Z, dA): # dZ = dA*(derivative of activation function at Z)\n",
    "    dZ = dA\n",
    "    return dZ\n",
    "\n",
    "def ReLU_der(Z, dA): # dZ = dA*(derivative of activation function at Z)\n",
    "    dZ = dA\n",
    "    dZ[dZ < 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def sig_der(Z, dA): # dZ = dA*(derivative of activation function at Z)\n",
    "    der_sig_at_Z = sigmoid(Z)*(1 - sigmoid(Z))\n",
    "    dZ = dA*(der_sig_at_Z)\n",
    "    return dZ\n",
    "\n",
    "def activation_fn_backprop(Z, dA, activation):\n",
    "    if activation == 'linear':\n",
    "        return lin_der(Z, dA)\n",
    "    elif activation == 'sigmoid':\n",
    "        return sig_der(Z, dA)\n",
    "    elif activation == 'ReLU':\n",
    "        return ReLU_der(Z, dA)\n",
    "    else:\n",
    "        print('Activation function not recognized.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-forward function. (Single layer)\n",
    "\n",
    "def feed_forward(A_prev, W_curr, b_curr, activation):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    A_curr = activation_fn(Z_curr, activation)\n",
    "    return (Z_curr, A_curr)\n",
    "\n",
    "# Forward propagation function. (Multi layered)\n",
    "\n",
    "def forward_prop(X, Y, parameters, ANN_architecture):\n",
    "    # Initializing cache and A_prev. \n",
    "    cache = {}\n",
    "    A_curr = X\n",
    "\n",
    "\n",
    "    # Run forward propagation over all layers.\n",
    "    for idx, layer in enumerate(ANN_architecture):\n",
    "        layer_idx = idx+1\n",
    "        A_prev = A_curr\n",
    "        W_curr = parameters['W' + str(layer_idx)]\n",
    "        b_curr = parameters['b' + str(layer_idx)]\n",
    "        activation = layer['activation']\n",
    "\n",
    "        Z_curr, A_curr = feed_forward(A_prev, W_curr, b_curr, activation) \n",
    "\n",
    "        cache['A' + str(idx)] = A_prev\n",
    "        cache['Z' + str(layer_idx)] = Z_curr\n",
    "    \n",
    "    error = error_fn(A_curr, Y)\n",
    "\n",
    "    return (error, cache, A_curr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation:\n",
    "\n",
    "- Given $W$, $b$, $\\hat{Y}$, $Y$ (expected output) and $X$, we find $\\frac{\\partial E}{\\partial W_{i_l, j_{l-1}}}$ and $\\frac{\\partial E}{\\partial b_{i_l}}$, where $E$ is the \"$\\textit{mean squared error}$\".\n",
    "- From gradient descent, $\\Delta W_{i_l, j_{l-1}} = -\\eta*\\frac{\\partial E}{\\partial W_{i_l, j_{l-1}}}$ and $\\Delta b_{i_l} = -\\eta*\\frac{\\partial E}{\\partial b_{i_l}}$, where $\\eta$ is the \"$\\textit{learning rate}$\". $\\eta$ is a hyperparameter of our choice.\n",
    "- Let $\\delta_{i_l} = \\frac{\\partial E}{\\partial Z_{i_l}}$. \n",
    "\n",
    "Here:\n",
    "- $\\delta_{L} = \\frac{\\partial E}{\\partial A_{L}} = y-\\hat{y}$\n",
    "- $\\frac{\\partial E}{\\partial W_{i_l, j_{l-1}}} =  \\frac{\\partial E}{\\partial A_{i_l}}\\frac{\\partial A_{i_l}}{\\partial Z_{i_l}}\\frac{\\partial Z_{i_l}}{\\partial W_{i_l, j_{l-1}}} = \\delta_{i_l}\\sigma'(Z_{i_l})A_{j_{l-1}}$.\n",
    "- $\\frac{\\partial E}{\\partial b_{i_l}} = \\frac{1}{N}\\sum_{i_{l} = 1}^{n_{l}}Z_{i_l}$\n",
    "- $\\Delta W_{i_l, j_{l-1}} = -\\eta\\delta_{i_l}\\sigma'(Z_{i_l})A_{j_{l-1}}$.\n",
    "- $\\Delta b_{i_l} = -\\eta\\frac{1}{N}\\sum_{i_{l} = 1}^{n_{l}}A_{i_l}$\n",
    "- $\\delta_{j_{l}} = \\sum_{i_{l+1} = 1}^{n_{l+1}}W_{i_{l+1}, j_{l}}\\sigma'(Z_{i_{l+1}})A_{j_{l-1}}$, for hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop function. (Single layer)\n",
    "\n",
    "def back_prop_single(dA_curr, Z_curr, A_prev, W_curr, activation): # delta = dE/dZ_next\n",
    "     # Normalization.\n",
    "     N = A_prev.shape[1]\n",
    "\n",
    "     # dA = dE/dA, dZ = dE/dZ, dW = dE/dW, db = dE/db.\n",
    "     dZ_curr = activation_fn_backprop(Z_curr, dA_curr, activation)\n",
    "     \n",
    "     dW_curr = np.dot(dZ_curr, A_prev.T)/N # dE/dW = dE/dX*dX/dW\n",
    "     db_curr = np.sum(dZ_curr, axis = 1, keepdims = True)/N #dE/db = dE/dX*dX/db\n",
    "     dA_prev = np.dot(W_curr.T, dZ_curr) # dE/dZ_prev = delta_prev\n",
    "\n",
    "     return dW_curr, db_curr, dA_prev\n",
    "\n",
    "# Backprop function. (Multiple layered.\n",
    "def back_prop(Y_hat, Y, parameters, cache, ANN_architecture):\n",
    "     #Initialize gradients and dA_prev.\n",
    "     gradients = {}\n",
    "     dA_prev = Y_hat - Y\n",
    "     \n",
    "     for layer_idx_prev, layer in reversed(list(enumerate(ANN_architecture))):\n",
    "          layer_idx_curr = layer_idx_prev+1\n",
    "          dA_curr = dA_prev\n",
    "\n",
    "          Z_curr = cache['Z' + str(layer_idx_curr)]\n",
    "          A_prev = cache['A' + str(layer_idx_prev)]\n",
    "          W_curr = parameters['W' + str(layer_idx_curr)]\n",
    "          activation_curr = layer['activation']\n",
    "\n",
    "          dW_curr, db_curr, dA_prev = back_prop_single(dA_curr, Z_curr, A_prev, W_curr, activation_curr)\n",
    "\n",
    "          gradients['dW' + str(layer_idx_curr)] = dW_curr\n",
    "          gradients['db' + str(layer_idx_curr)] = db_curr\n",
    "     \n",
    "     return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Update function.\n",
    "\n",
    "def update(ANN_architecture, parameters, gradients, eta):\n",
    "    # Update parameters.\n",
    "    for layer_idx, _ in enumerate(ANN_architecture, 1):\n",
    "        parameters['W' + str(layer_idx)] -= eta * gradients['dW' + str(layer_idx)]        \n",
    "        parameters['b' + str(layer_idx)] -= eta * gradients['db' + str(layer_idx)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Perturbation:\n",
    "\n",
    "- we perturb weight weight $W_{i_l, j_{l-1}}$ by $pert_{i_l, j_{l-1}}$. \n",
    "- Next we find $y$ using forward propagation. Then, we find $\\frac{\\partial E}{\\partial W_{i_l, j_{l-1}}} = \\frac{E(W_{i_l, j_{l-1}} + pert_{i_l, j_{l-1}}) - E(W_{i_l, j_{l-1}})}{pert_{i_l, j_{l-1}}}$.\n",
    "- Next, $\\Delta W_{i_l, j_{l-1}} = -\\eta*\\frac{\\partial E}{\\partial W_{i_l, j_{l-1}}}$, as calculated above.\n",
    "- We repeat the above process for all $W_{i_l, j_{l-1}}$ while the error is greater than the convergence criteria. (0.01 in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Perterbation function. (One round of perturbation)\n",
    "\n",
    "def weight_pert(parameters, X, Y, E, pert, ANN_architecture): # pert is the perterbation strength. conv is the convergence criteria\n",
    "    # Initialize gradients and perturbed parameters.\n",
    "    gradients = {}\n",
    "    parameters_pert = parameters\n",
    "    for layer_idx, layer in enumerate(ANN_architecture, 1):\n",
    "        gradients['dW' + str(layer_idx)] = np.zeros(parameters['W' + str(layer_idx)].shape)\n",
    "        gradients['db' + str(layer_idx)] = np.zeros(parameters['b' + str(layer_idx)].shape)\n",
    "\n",
    "    # Run weight perturbation.\n",
    "    for layer_idx, layer in enumerate(ANN_architecture, 1):\n",
    "        for weight_idx, _ in np.ndenumerate(parameters['W' + str(layer_idx)]):\n",
    "            E_pert = 0\n",
    "            parameters_pert['W' + str(layer_idx)][weight_idx] += pert\n",
    "            E_pert, _, _ = forward_prop(X, Y, parameters_pert, ANN_architecture)\n",
    "            del_E = E_pert - E\n",
    "            gradients['dW' + str(layer_idx)][weight_idx] = del_E/pert\n",
    "            #print(gradients['dW' + str(layer_idx)][weight_idx])\n",
    "        for bias_idx, _ in np.ndenumerate(parameters['b' + str(layer_idx)]):\n",
    "            parameters_pert['b' + str(layer_idx)][bias_idx] += pert\n",
    "            E_pert, _, _ = forward_prop(X, Y, parameters_pert, ANN_architecture)\n",
    "            del_E = E_pert - E\n",
    "            gradients['db' + str(layer_idx)][bias_idx] = del_E/pert\n",
    "\n",
    "\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ANN:\n",
    " \n",
    "We define a train function which takes in the ANN_architecture, X, $\\hat{Y}$, epochs and eta and returns the output after training and the error after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training using backprop.\n",
    " \n",
    "def train_backprop(ANN_architecture, X, Y, epochs, eta):\n",
    "    # Initialize weights and errors.\n",
    "    parameters = init_ANN(ANN_architecture, 1)\n",
    "    errors = []\n",
    "\n",
    "    # Train.\n",
    "    for ep in range(epochs):\n",
    "        error, cache, Y_hat = forward_prop(X, Y, parameters, ANN_architecture)\n",
    "        errors.append(error)\n",
    "\n",
    "        gradients = back_prop(Y_hat, Y, parameters, cache, ANN_architecture)\n",
    "        parameters = update(ANN_architecture, parameters, gradients, eta)\n",
    "\n",
    "    return parameters, np.asarray(errors)\n",
    "\n",
    "# Training using weight perturbation.\n",
    "\n",
    "def train_pert(ANN_architecture, X, Y, epochs, pert, eta):\n",
    "    # Initialize weights and errors.\n",
    "    parameters = init_ANN(ANN_architecture, 1)\n",
    "    errors = []\n",
    "\n",
    "    # Train.\n",
    "    for ep in range(epochs):\n",
    "        error, cache, Y_hat = forward_prop(X, Y, parameters, ANN_architecture)\n",
    "        errors.append(error)\n",
    "\n",
    "        gradients = weight_pert(parameters, X, Y, error, pert, ANN_architecture)\n",
    "        parameters = update(ANN_architecture, parameters, gradients, eta)\n",
    "\n",
    "    return parameters, np.asarray(errors)\n",
    "\n",
    "# Testing.\n",
    "\n",
    "def test(X, Y, parameters, ANN_architecture):\n",
    "    error, _, Y = forward_prop(X, Y, parameters, ANN_architecture)\n",
    "    Y[Y > 0.5] = 1\n",
    "    Y[Y <= 0.5] = 0\n",
    "    return Y, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward XOR network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-forward XOR. (Main 1)\n",
    "\n",
    "ANN_architecture = [{'inputs':2, 'outputs':2, 'activation':'sigmoid'}, {'inputs':2, 'outputs':1, 'activation':'linear'}]\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) \n",
    "Y = np.array([[0, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1.]]\n",
      "[[1. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b6089b2490>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters_bp, errors_bp = train_backprop(ANN_architecture, X, Y, 3000, 0.3)\n",
    "Y_bp, E_bp = test(np.array([[1, 1, 0, 0], [0, 1, 0, 1]]), np.array([[1, 0, 0, 1]]), parameters_bp, ANN_architecture)\n",
    "plt.plot(range(3000), errors_bp)\n",
    "print(Y_bp)\n",
    "parameters_pert, errors_pert = train_pert(ANN_architecture, X, Y, 3000, 0.0001, 0.3)\n",
    "Y_pert, E_pert = test(np.array([[1, 1, 0, 0], [0, 1, 0, 1]]), np.array([[1, 0, 0, 1]]), parameters_pert, ANN_architecture)\n",
    "print(Y_bp)\n",
    "plt.plot(range(3000), errors_pert)\n",
    "plt.legend(['backprop err', 'weight pert err'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "096806d1539b157da8138530fbb8d38015718dd9cd3706ea38d3c1d7b06ac51f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
